{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install tensorflow[and-cuda]==2.15.0.post1 transformers==4.36.2 einops==0.7.0 datasets==2.16.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-14T11:43:31.218682Z","iopub.execute_input":"2024-08-14T11:43:31.219364Z","iopub.status.idle":"2024-08-14T11:46:37.461824Z","shell.execute_reply.started":"2024-08-14T11:43:31.219332Z","shell.execute_reply":"2024-08-14T11:46:37.460636Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.15.0.post1 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting transformers==4.36.2\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting einops==0.7.0\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nCollecting datasets==2.16.1\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.23.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2.32.3)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (4.66.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.1)\nCollecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.16.5 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.42.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2024.7.4)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.0.3)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.1)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.2.2)\nDownloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m202.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl (417.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.8/417.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl (13.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (23.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (845 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m845.8/845.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl (720.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.1/720.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl (98.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl (124.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl (195.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.3/195.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, keras, fsspec, einops, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tokenizers, nvidia-cusolver-cu12, transformers, datasets, tensorflow\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.5.0\n    Uninstalling fsspec-2024.5.0:\n      Successfully uninstalled fsspec-2024.5.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.1 dill-0.3.7 einops-0.7.0 fsspec-2023.10.0 keras-2.15.0 multiprocess-0.70.15 nvidia-cublas-cu12-12.2.5.6 nvidia-cuda-cupti-cu12-12.2.142 nvidia-cuda-nvcc-cu12-12.2.140 nvidia-cuda-nvrtc-cu12-12.2.140 nvidia-cuda-runtime-cu12-12.2.140 nvidia-cudnn-cu12-8.9.4.25 nvidia-cufft-cu12-11.0.8.103 nvidia-curand-cu12-10.3.3.141 nvidia-cusolver-cu12-11.5.2.141 nvidia-cusparse-cu12-12.1.2.141 nvidia-nccl-cu12-2.16.5 nvidia-nvjitlink-cu12-12.2.140 tensorflow-2.15.0.post1 tokenizers-0.15.2 transformers-4.36.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\n\nfrom dataclasses import dataclass\nfrom einops import rearrange, repeat\nfrom typing import Union\n\nfrom transformers import AutoTokenizer\n\nimport datasets\nimport math\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-14T11:46:37.463542Z","iopub.execute_input":"2024-08-14T11:46:37.463879Z","iopub.status.idle":"2024-08-14T11:46:49.525334Z","shell.execute_reply.started":"2024-08-14T11:46:37.463850Z","shell.execute_reply":"2024-08-14T11:46:49.524403Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-08-14 11:46:39.722391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-14 11:46:39.722533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-14 11:46:39.725673: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"max_seq_length = 512","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:18:13.023008Z","iopub.execute_input":"2024-08-14T12:18:13.023392Z","iopub.status.idle":"2024-08-14T12:18:13.027608Z","shell.execute_reply.started":"2024-08-14T12:18:13.023364Z","shell.execute_reply":"2024-08-14T12:18:13.026624Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelArgs:\n    model_input_dims: int = 64\n    model_states: int = 64\n    projection_expand_factor: int = 2\n    conv_kernel_size: int = 4\n    delta_t_min: float = 0.001\n    delta_t_max: float = 0.1\n    delta_t_scale: float = 0.1\n    delta_t_init_floor: float = 1e-4\n    conv_use_bias: bool = True\n    dense_use_bias: bool = False\n    layer_id: int = -1\n    seq_length: int = max_seq_length\n    num_layers: int = 5\n    dropout_rate: float = 0.2\n    use_lm_head: float = False\n    num_classes: int = None\n    vocab_size: int = None\n    final_activation = None\n    loss:Union[str, keras.losses.Loss] = None\n    optimizer: Union[str, keras.optimizers.Optimizer] = keras.optimizers.AdamW()\n    metrics = ['accuracy']\n\n    def __post_init__(self):\n        self.model_internal_dim: int = int(self.projection_expand_factor * self.model_input_dims)\n\n        self.delta_t_rank = math.ceil(self.model_input_dims/16)\n        if self.layer_id == -1:\n            self.layer_id = np.round(np.random.randint(0, 1000), 4)\n\n        if self.vocab_size == None:\n            raise ValueError(\"vocab size cannot be none\")\n\n        if self.use_lm_head:\n            self.num_classes=self.vocab_size\n        else:\n            if self.num_classes == None:\n                raise ValueError(f'num classes cannot be {self.num_classes}')\n\n            if self.num_classes == 1:\n                self.final_activation = 'sigmoid'\n            else:\n                self.final_activation = 'softmax'\n\n        if self.loss == None:\n            raise ValueError(f\"loss cannot be {self.loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:18:13.201975Z","iopub.execute_input":"2024-08-14T12:18:13.202293Z","iopub.status.idle":"2024-08-14T12:18:13.217393Z","shell.execute_reply.started":"2024-08-14T12:18:13.202268Z","shell.execute_reply":"2024-08-14T12:18:13.216452Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\nvocab_size = tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-08-14T11:48:30.395679Z","iopub.execute_input":"2024-08-14T11:48:30.396014Z","iopub.status.idle":"2024-08-14T11:48:31.410850Z","shell.execute_reply.started":"2024-08-14T11:48:30.395987Z","shell.execute_reply":"2024-08-14T11:48:31.409975Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63582fd768df4e12bbbe5bf7bb030f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cddb696b2bb4f84bfa65fd024cd726d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def selective_scan(u, delta, A, B, C, D):\n    # first step of A_bar = exp(ΔA), i.e., ΔA\n    dA = tf.einsum('bld,dn->bldn', delta, A) \n    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)\n    \n    dA_cumsum = tf.pad(\n        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n    \n    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1\n    \n    # Cumulative sum along all the input tokens, parallel prefix sum, \n    # calculates dA for all the input tokens parallely\n    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  \n\n    # second step of A_bar = exp(ΔA), i.e., exp(ΔA)\n    dA_cumsum = tf.exp(dA_cumsum)  \n    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1\n\n    x = dB_u * dA_cumsum\n    # 1e-12 to avoid division by 0\n    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) \n\n    y = tf.einsum('bldn,bln->bld', x, C)\n    \n    return y + u * D ","metadata":{"execution":{"iopub.status.busy":"2024-08-14T11:48:40.828104Z","iopub.execute_input":"2024-08-14T11:48:40.828739Z","iopub.status.idle":"2024-08-14T11:48:40.836957Z","shell.execute_reply.started":"2024-08-14T11:48:40.828706Z","shell.execute_reply":"2024-08-14T11:48:40.835897Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MambaBlock(layers.Layer):\n    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.args = modelargs\n        args = modelargs\n        self.layer_id = modelargs.layer_id\n\n        self.in_projection = layers.Dense(\n            args.model_internal_dim * 2, \n            input_shape=(args.model_input_dims,), use_bias=False)\n\n        self.conv1d = layers.Conv1D(\n            filters=args.model_internal_dim,\n            use_bias=args.conv_use_bias,\n            kernel_size=args.conv_kernel_size,\n            groups=args.model_internal_dim,\n            data_format='channels_first',\n            padding='causal'\n        )\n\n        # this layer takes in current token 'x' \n        # and outputs the input-specific Δ, B, C (according to S6)\n        self.x_projection = layers.Dense(args.delta_t_rank + args.model_states * 2, use_bias=False)\n\n        # this layer projects Δ from delta_t_rank to the mamba internal \n        # dimension\n        self.delta_t_projection = layers.Dense(args.model_internal_dim, \n                                               input_shape=(args.delta_t_rank,), use_bias=True)\n\n        self.A = repeat(\n                tf.range(1, args.model_states+1, dtype=tf.float32), \n                'n -> d n', d=args.model_internal_dim)\n\n        self.A_log = tf.Variable(\n                tf.math.log(self.A), \n                trainable=True, dtype=tf.float32, \n                name=f\"SSM_A_log_{args.layer_id}\")\n\n        self.D = tf.Variable(\n                np.ones(args.model_internal_dim), \n                trainable=True, dtype=tf.float32, \n                name=f\"SSM_D_{args.layer_id}\")\n\n        self.out_projection = layers.Dense(\n                args.model_input_dims, \n                input_shape=(args.model_internal_dim,), \n                use_bias=args.dense_use_bias)\n\n    def call(self, x):\n        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba pape.\n        Official Implementation:\n            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n        \"\"\"\n\n        (batch_size, seq_len, dimension) = x.shape\n\n        x_and_res = self.in_projection(x) # shape = (batch, seq_len, 2 * model_internal_dimension)\n        (x, res) = tf.split(x_and_res, \n                            [self.args.model_internal_dim, \n                             self.args.model_internal_dim], axis=-1)\n        \n        x = rearrange(x, 'b l d_in -> b d_in l')\n        x = self.conv1d(x)[:, :, :seq_len]\n        x = rearrange(x, 'b d_in l -> b l d_in')\n        \n        x = tf.nn.swish(x)\n        y = self.ssm(x)\n        y = y * tf.nn.swish(res)\n        return self.out_projection(y)\n    \n    def ssm(self, x):\n        \"\"\"Runs the SSM. See:\n            - Algorithm 2 in Section 3.2 in the Mamba paper\n            - run_SSM(A, B, C, u) in The Annotated S4\n            Official Implementation:\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n        \"\"\"\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n        #                                  and is why Mamba is called **selective** state spaces)\n\n        A = -tf.exp(tf.cast(self.A_log, tf.float32)) # shape -> (d_in, n)\n        D = tf.cast(self.D, tf.float32)\n\n        x_dbl = self.x_projection(x) # shape -> (batch, seq_len, delta_t_rank + 2*n)\n\n        (delta, B, C) = tf.split(\n                x_dbl, \n                num_or_size_splits=[self.args.delta_t_rank, n, n], \n                axis=-1) # delta.shape -> (batch, seq_len) & B, C shape -> (batch, seq_len, n)\n\n        delta = tf.nn.softplus(self.delta_t_projection(delta)) # shape -> (batch, seq_len, model_input_dim)\n\n        return selective_scan(x, delta, A, B, C, D)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T11:49:34.094821Z","iopub.execute_input":"2024-08-14T11:49:34.095664Z","iopub.status.idle":"2024-08-14T11:49:34.112806Z","shell.execute_reply.started":"2024-08-14T11:49:34.095632Z","shell.execute_reply":"2024-08-14T11:49:34.111781Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(layers.Layer):\n    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.args = modelargs\n        self.mixer = MambaBlock(modelargs)\n        self.norm = layers.LayerNormalization(epsilon=1e-5)\n\n    def call(self, x):\n        \"\"\"\n        Official Implementation:\n            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n            \n            Note: the official repo chains residual blocks that look like\n                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n            where the first Add is a no-op. This is purely for performance reasons as this\n            allows them to fuse the Add->Norm.\n\n            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n            \n        \"\"\"\n        return self.mixer(self.norm(x)) + x","metadata":{"execution":{"iopub.status.busy":"2024-08-14T11:49:47.362274Z","iopub.execute_input":"2024-08-14T11:49:47.363002Z","iopub.status.idle":"2024-08-14T11:49:47.370488Z","shell.execute_reply.started":"2024-08-14T11:49:47.362966Z","shell.execute_reply":"2024-08-14T11:49:47.369118Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def init_model(args: ModelArgs):\n    input_layer = layers.Input(shape=(args.seq_length,), name='input_ids')\n    x = layers.Embedding(args.vocab_size, args.model_input_dims, input_length=args.seq_length)(input_layer)\n\n    for i in range(args.num_layers):\n        x = ResidualBlock(args, name=f\"Residual_{i}\")(x)\n        x = layers.Dropout(args.dropout_rate)(x)\n\n    x = layers.LayerNormalization(epsilon=1e-5)(x)\n\n    if not args.use_lm_head: # use flatten only if we are using the model as an LM\n        x = layers.Flatten()(x)\n    x = layers.Dense(1024, activation=tf.nn.gelu)(x)\n    output_layer = layers.Dense(args.num_classes, activation=args.final_activation)(x)\n\n    model = Model(inputs=input_layer, outputs=output_layer, name='MambaTimeModel')\n    model.compile(\n        loss=args.loss,\n        optimizer=args.optimizer,\n        metrics=args.metrics\n    )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-14T11:51:05.377090Z","iopub.execute_input":"2024-08-14T11:51:05.377461Z","iopub.status.idle":"2024-08-14T11:51:05.386422Z","shell.execute_reply.started":"2024-08-14T11:51:05.377432Z","shell.execute_reply":"2024-08-14T11:51:05.385412Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"args = ModelArgs(\n    model_input_dims=max_seq_length,\n    model_states=32,\n    num_layers=12,\n    dropout_rate=0.2,\n    vocab_size=vocab_size,\n    num_classes=1,\n    loss='binary_crossentropy',\n)\nmodel = init_model(args)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:18:18.803239Z","iopub.execute_input":"2024-08-14T12:18:18.803571Z","iopub.status.idle":"2024-08-14T12:18:20.930965Z","shell.execute_reply.started":"2024-08-14T12:18:18.803533Z","shell.execute_reply":"2024-08-14T12:18:20.930092Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Model: \"MambaTimeModel\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_ids (InputLayer)      [(None, 512)]             0         \n                                                                 \n embedding_3 (Embedding)     (None, 512, 512)          25730048  \n                                                                 \n Residual_0 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_36 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_1 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_37 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_2 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_38 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_3 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_39 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_4 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_40 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_5 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_41 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_6 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_42 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_7 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_43 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_8 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_44 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_9 (ResidualBlock)  (None, 512, 512)          1744896   \n                                                                 \n dropout_45 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_10 (ResidualBlock  (None, 512, 512)          1744896   \n )                                                               \n                                                                 \n dropout_46 (Dropout)        (None, 512, 512)          0         \n                                                                 \n Residual_11 (ResidualBlock  (None, 512, 512)          1744896   \n )                                                               \n                                                                 \n dropout_47 (Dropout)        (None, 512, 512)          0         \n                                                                 \n layer_normalization_51 (La  (None, 512, 512)          1024      \n yerNormalization)                                               \n                                                                 \n flatten_3 (Flatten)         (None, 262144)            0         \n                                                                 \n dense_198 (Dense)           (None, 1024)              268436480 \n                                                                 \n dense_199 (Dense)           (None, 1)                 1025      \n                                                                 \n=================================================================\nTotal params: 315107329 (1.17 GB)\nTrainable params: 315107329 (1.17 GB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tqdm import tqdm\n\ndataset = load_dataset(\"neuralwork/fashion-style-instruct\", split=\"train\")\ndataset = dataset.train_test_split(test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:02.573124Z","iopub.execute_input":"2024-08-14T12:16:02.573393Z","iopub.status.idle":"2024-08-14T12:16:03.691839Z","shell.execute_reply.started":"2024-08-14T12:16:02.573369Z","shell.execute_reply":"2024-08-14T12:16:03.691008Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:03.692841Z","iopub.execute_input":"2024-08-14T12:16:03.693102Z","iopub.status.idle":"2024-08-14T12:16:03.698703Z","shell.execute_reply.started":"2024-08-14T12:16:03.693078Z","shell.execute_reply":"2024-08-14T12:16:03.697831Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'completion', 'context'],\n        num_rows: 2235\n    })\n    test: Dataset({\n        features: ['input', 'completion', 'context'],\n        num_rows: 958\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token \nEOS_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:03.700635Z","iopub.execute_input":"2024-08-14T12:16:03.700915Z","iopub.status.idle":"2024-08-14T12:16:03.708617Z","shell.execute_reply.started":"2024-08-14T12:16:03.700885Z","shell.execute_reply":"2024-08-14T12:16:03.707783Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}]},{"cell_type":"code","source":"alpacaFormatString = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Context:\n{}\n\n### Input:\n{}\n\n### Completion:\n{}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:03.709790Z","iopub.execute_input":"2024-08-14T12:16:03.710112Z","iopub.status.idle":"2024-08-14T12:16:03.716407Z","shell.execute_reply.started":"2024-08-14T12:16:03.710082Z","shell.execute_reply":"2024-08-14T12:16:03.715529Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Define the function to create the new 'text' column\ndef formatDatasetAlpaca(sample):\n    context = sample['context']\n    inputText = sample['input']\n    completion = sample['completion']\n    \n    text = alpacaFormatString.format(context, inputText, completion) + EOS_TOKEN\n    sample['text'] = text\n    \n    return sample\n\n# Apply the function to the dataset\ndataset = dataset.map(formatDatasetAlpaca)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:03.717521Z","iopub.execute_input":"2024-08-14T12:16:03.717855Z","iopub.status.idle":"2024-08-14T12:16:04.246325Z","shell.execute_reply.started":"2024-08-14T12:16:03.717833Z","shell.execute_reply":"2024-08-14T12:16:04.245475Z"},"trusted":true},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2235 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d53af84730c49a6b623062bd120de77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/958 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ee81d7205464f8e817b8e4ba20e0876"}},"metadata":{}}]},{"cell_type":"code","source":"import pprint\npprint.pprint(dataset['train'][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:04.247498Z","iopub.execute_input":"2024-08-14T12:16:04.247858Z","iopub.status.idle":"2024-08-14T12:16:04.257125Z","shell.execute_reply.started":"2024-08-14T12:16:04.247825Z","shell.execute_reply":"2024-08-14T12:16:04.256234Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"{'completion': 'Outfit 1:\\n'\n               '- Top: A sleeveless, sequin-embellished blouse in a neutral '\n               'color, such as silver or blush. This will add a touch of '\n               'glamour and sophistication to your outfit.\\n'\n               '- Bottom: Pair the blouse with high-waisted, tapered black '\n               'trousers. The high waist will elongate your legs and create a '\n               'streamlined silhouette.\\n'\n               '- Shoes: Opt for a pair of sleek black pointed-toe pumps. This '\n               'will add height and elegance to your overall look.\\n'\n               '- Accessories: Finish the outfit with a statement chunky '\n               'necklace in silver or rose gold. Add a small clutch in a '\n               'contrasting color, such as black or metallic, to hold your '\n               'essentials.\\n'\n               '\\n'\n               'Outfit 2:\\n'\n               '- Top: A lace peplum top in a rich jewel tone, such as emerald '\n               'or sapphire. The peplum detail will add a feminine touch to '\n               'your outfit.\\n'\n               '- Bottom: Choose a pair of ankle-length tailored trousers in a '\n               'complementary color, like black or navy. These will create a '\n               'polished and sophisticated look.\\n'\n               '- Shoes: Opt for a pair of nude or metallic strappy heels. '\n               'These will elongate your legs and give a subtle glam touch.\\n'\n               '- Accessories: Add a touch of sparkle with a pair of dainty '\n               'drop earrings. Carry a structured clutch in a coordinating '\n               'color to complete the ensemble.\\n'\n               '\\n'\n               'Outfit 3:\\n'\n               '- Top: A silky blouse in a bold, vibrant print, like a floral '\n               'or abstract pattern. This will add a modern and fashionable '\n               'element to your look.\\n'\n               '- Bottom: Pair the blouse with slim-fitting black trousers '\n               'with a slight flare at the bottom. This will balance the '\n               'volume of the top and create a sleek silhouette.\\n'\n               '- Shoes: Opt for a pair of pointed-toe ankle boots in a '\n               'neutral color, like black or taupe. These will add an edgy '\n               'touch to the outfit while still maintaining sophistication.\\n'\n               '- Accessories: Keep accessories minimal with a delicate '\n               'pendant necklace and a stack of simple bangles. Carry a '\n               'structured minaudiere clutch in a coordinating color or '\n               'metallic for a touch of elegance.\\n'\n               '\\n'\n               'Outfit 4:\\n'\n               '- Top: A tailored blazer in a classic color, like navy or '\n               'charcoal gray. Opt for one with feminine details, such as a '\n               'peplum waist or ruched sleeves, to add a modern twist.\\n'\n               '- Bottom: Pair the blazer with high-waisted tailored trousers '\n               'in a coordinating color. This will create a chic and '\n               'put-together look.\\n'\n               '- Shoes: Go for a pair of pointy-toe kitten heels in a '\n               'matching hue. These will add a touch of sophistication and '\n               'comfort for a longer event.\\n'\n               '- Accessories: Add a statement brooch to the blazer lapel to '\n               'elevate the look. Opt for a metallic or embellished clutch to '\n               'complement the outfit.\\n'\n               '\\n'\n               'Outfit 5:\\n'\n               '- Top: A chiffon wrap blouse in a soft pastel color, such as '\n               'blush or lavender. The wrap silhouette will flatter your '\n               'figure and add a touch of femininity.\\n'\n               '- Bottom: Pair the blouse with a high-waisted, wide-leg pant '\n               'in a complementary color, like cream or light gray. This '\n               'combination will create an elegant and flowy silhouette.\\n'\n               '- Shoes: Opt for a pair of embellished flat sandals in '\n               'metallic or jewel tones. These will add a playful and '\n               'comfortable element to your ensemble.\\n'\n               '- Accessories: Add a delicate chain belt to cinch in the waist '\n               'and enhance your figure. Finish the look with a small '\n               'crossbody bag in a neutral tone to keep your essentials handy.',\n 'context': \"I'm going to a gala / exhibition opening.\",\n 'input': \"I'm a petite, mature woman. I look for sophisticated, \"\n          'age-appropriate styles that are both modern and flattering, like '\n          'tailored blazers and tapered trousers.',\n 'text': 'Below is an instruction that describes a task, paired with an input '\n         'that provides further context. Write a response that appropriately '\n         'completes the request.\\n'\n         '\\n'\n         '### Context:\\n'\n         \"I'm going to a gala / exhibition opening.\\n\"\n         '\\n'\n         '### Input:\\n'\n         \"I'm a petite, mature woman. I look for sophisticated, \"\n         'age-appropriate styles that are both modern and flattering, like '\n         'tailored blazers and tapered trousers.\\n'\n         '\\n'\n         '### Completion:\\n'\n         'Outfit 1:\\n'\n         '- Top: A sleeveless, sequin-embellished blouse in a neutral color, '\n         'such as silver or blush. This will add a touch of glamour and '\n         'sophistication to your outfit.\\n'\n         '- Bottom: Pair the blouse with high-waisted, tapered black trousers. '\n         'The high waist will elongate your legs and create a streamlined '\n         'silhouette.\\n'\n         '- Shoes: Opt for a pair of sleek black pointed-toe pumps. This will '\n         'add height and elegance to your overall look.\\n'\n         '- Accessories: Finish the outfit with a statement chunky necklace in '\n         'silver or rose gold. Add a small clutch in a contrasting color, such '\n         'as black or metallic, to hold your essentials.\\n'\n         '\\n'\n         'Outfit 2:\\n'\n         '- Top: A lace peplum top in a rich jewel tone, such as emerald or '\n         'sapphire. The peplum detail will add a feminine touch to your '\n         'outfit.\\n'\n         '- Bottom: Choose a pair of ankle-length tailored trousers in a '\n         'complementary color, like black or navy. These will create a '\n         'polished and sophisticated look.\\n'\n         '- Shoes: Opt for a pair of nude or metallic strappy heels. These '\n         'will elongate your legs and give a subtle glam touch.\\n'\n         '- Accessories: Add a touch of sparkle with a pair of dainty drop '\n         'earrings. Carry a structured clutch in a coordinating color to '\n         'complete the ensemble.\\n'\n         '\\n'\n         'Outfit 3:\\n'\n         '- Top: A silky blouse in a bold, vibrant print, like a floral or '\n         'abstract pattern. This will add a modern and fashionable element to '\n         'your look.\\n'\n         '- Bottom: Pair the blouse with slim-fitting black trousers with a '\n         'slight flare at the bottom. This will balance the volume of the top '\n         'and create a sleek silhouette.\\n'\n         '- Shoes: Opt for a pair of pointed-toe ankle boots in a neutral '\n         'color, like black or taupe. These will add an edgy touch to the '\n         'outfit while still maintaining sophistication.\\n'\n         '- Accessories: Keep accessories minimal with a delicate pendant '\n         'necklace and a stack of simple bangles. Carry a structured '\n         'minaudiere clutch in a coordinating color or metallic for a touch of '\n         'elegance.\\n'\n         '\\n'\n         'Outfit 4:\\n'\n         '- Top: A tailored blazer in a classic color, like navy or charcoal '\n         'gray. Opt for one with feminine details, such as a peplum waist or '\n         'ruched sleeves, to add a modern twist.\\n'\n         '- Bottom: Pair the blazer with high-waisted tailored trousers in a '\n         'coordinating color. This will create a chic and put-together look.\\n'\n         '- Shoes: Go for a pair of pointy-toe kitten heels in a matching hue. '\n         'These will add a touch of sophistication and comfort for a longer '\n         'event.\\n'\n         '- Accessories: Add a statement brooch to the blazer lapel to elevate '\n         'the look. Opt for a metallic or embellished clutch to complement the '\n         'outfit.\\n'\n         '\\n'\n         'Outfit 5:\\n'\n         '- Top: A chiffon wrap blouse in a soft pastel color, such as blush '\n         'or lavender. The wrap silhouette will flatter your figure and add a '\n         'touch of femininity.\\n'\n         '- Bottom: Pair the blouse with a high-waisted, wide-leg pant in a '\n         'complementary color, like cream or light gray. This combination will '\n         'create an elegant and flowy silhouette.\\n'\n         '- Shoes: Opt for a pair of embellished flat sandals in metallic or '\n         'jewel tones. These will add a playful and comfortable element to '\n         'your ensemble.\\n'\n         '- Accessories: Add a delicate chain belt to cinch in the waist and '\n         'enhance your figure. Finish the look with a small crossbody bag in a '\n         'neutral tone to keep your essentials handy.<|endoftext|>'}\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:04.258255Z","iopub.execute_input":"2024-08-14T12:16:04.258526Z","iopub.status.idle":"2024-08-14T12:16:04.299946Z","shell.execute_reply.started":"2024-08-14T12:16:04.258503Z","shell.execute_reply":"2024-08-14T12:16:04.299115Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'completion', 'context', 'text'],\n        num_rows: 2235\n    })\n    test: Dataset({\n        features: ['input', 'completion', 'context', 'text'],\n        num_rows: 958\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Extract the \"text\" column for train and test datasets\ntrain_texts = dataset['train']['text']\ntest_texts = dataset['test']['text']","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:16:04.301136Z","iopub.execute_input":"2024-08-14T12:16:04.301490Z","iopub.status.idle":"2024-08-14T12:16:04.317270Z","shell.execute_reply.started":"2024-08-14T12:16:04.301458Z","shell.execute_reply":"2024-08-14T12:16:04.316473Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Tokenize the texts\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_seq_length)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:19:17.812169Z","iopub.execute_input":"2024-08-14T12:19:17.812856Z","iopub.status.idle":"2024-08-14T12:19:20.198644Z","shell.execute_reply.started":"2024-08-14T12:19:17.812822Z","shell.execute_reply":"2024-08-14T12:19:20.197616Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Convert lists to tensors\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings)\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings)\n))\n\n# Batch and shuffle the datasets\nBATCH_SIZE = 32\ntrain_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:19:20.200438Z","iopub.execute_input":"2024-08-14T12:19:20.200753Z","iopub.status.idle":"2024-08-14T12:19:29.585577Z","shell.execute_reply.started":"2024-08-14T12:19:20.200728Z","shell.execute_reply":"2024-08-14T12:19:29.584794Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset, validation_data=test_dataset, epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:19:29.586662Z","iopub.execute_input":"2024-08-14T12:19:29.586941Z","iopub.status.idle":"2024-08-14T12:19:32.328151Z","shell.execute_reply.started":"2024-08-14T12:19:29.586918Z","shell.execute_reply":"2024-08-14T12:19:32.326921Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['attention_mask'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_file5e0ha4xt.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1152, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1106, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=binary_crossentropy, and therefore expects target data to be provided in `fit()`.\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1152, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1106, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=binary_crossentropy, and therefore expects target data to be provided in `fit()`.\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}