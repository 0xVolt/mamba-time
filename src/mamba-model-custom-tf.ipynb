{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n%pip install tensorflow[and-cuda]==2.15.0.post1 transformers==4.36.2 einops==0.7.0 datasets==2.16.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-14T15:28:32.878793Z","iopub.execute_input":"2024-08-14T15:28:32.879184Z","iopub.status.idle":"2024-08-14T15:37:07.751792Z","shell.execute_reply.started":"2024-08-14T15:28:32.879152Z","shell.execute_reply":"2024-08-14T15:37:07.750669Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting tensorflow==2.15.0.post1 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting transformers==4.36.2\n  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\nCollecting einops==0.7.0\n  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nCollecting datasets==2.16.1\n  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1)\n  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.23.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2.32.3)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (4.66.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.1)\nCollecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.16.5 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.2.140 (from tensorflow[and-cuda]==2.15.0.post1)\n  Using cached nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.42.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2024.7.4)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.0.3)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.1)\n  Using cached multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1->tensorflow[and-cuda]==2.15.0.post1) (3.2.2)\nUsing cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\nUsing cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\nUsing cached einops-0.7.0-py3-none-any.whl (44 kB)\nUsing cached datasets-2.16.1-py3-none-any.whl (507 kB)\nUsing cached nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl (417.8 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl (13.9 MB)\nUsing cached nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (21.3 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (23.4 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (845 kB)\nDownloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl (720.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.1/720.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl (98.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl (124.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl (195.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.3/195.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, keras, fsspec, einops, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tokenizers, nvidia-cusolver-cu12, transformers, datasets, tensorflow\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.5.0\n    Uninstalling fsspec-2024.5.0:\n      Successfully uninstalled fsspec-2024.5.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.1 dill-0.3.7 einops-0.7.0 fsspec-2023.10.0 keras-2.15.0 multiprocess-0.70.15 nvidia-cublas-cu12-12.2.5.6 nvidia-cuda-cupti-cu12-12.2.142 nvidia-cuda-nvcc-cu12-12.2.140 nvidia-cuda-nvrtc-cu12-12.2.140 nvidia-cuda-runtime-cu12-12.2.140 nvidia-cudnn-cu12-8.9.4.25 nvidia-cufft-cu12-11.0.8.103 nvidia-curand-cu12-10.3.3.141 nvidia-cusolver-cu12-11.5.2.141 nvidia-cusparse-cu12-12.1.2.141 nvidia-nccl-cu12-2.16.5 nvidia-nvjitlink-cu12-12.2.140 tensorflow-2.15.0.post1 tokenizers-0.15.2 transformers-4.36.2\nNote: you may need to restart the kernel to use updated packages.\nCPU times: user 10.8 s, sys: 2.36 s, total: 13.2 s\nWall time: 8min 34s\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\n\nfrom dataclasses import dataclass\nfrom einops import rearrange, repeat\nfrom typing import Union\n\nfrom transformers import AutoTokenizer\n\nimport datasets\nimport math\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:37:07.754032Z","iopub.execute_input":"2024-08-14T15:37:07.754883Z","iopub.status.idle":"2024-08-14T15:37:12.004974Z","shell.execute_reply.started":"2024-08-14T15:37:07.754852Z","shell.execute_reply":"2024-08-14T15:37:12.004234Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 512","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:37:12.006010Z","iopub.execute_input":"2024-08-14T15:37:12.006595Z","iopub.status.idle":"2024-08-14T15:37:12.010586Z","shell.execute_reply.started":"2024-08-14T15:37:12.006568Z","shell.execute_reply":"2024-08-14T15:37:12.009755Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelArgs:\n    model_input_dims: int = 64\n    model_states: int = 64\n    projection_expand_factor: int = 2\n    conv_kernel_size: int = 4\n    delta_t_min: float = 0.001\n    delta_t_max: float = 0.1\n    delta_t_scale: float = 0.1\n    delta_t_init_floor: float = 1e-4\n    conv_use_bias: bool = True\n    dense_use_bias: bool = False\n    layer_id: int = -1\n    seq_length: int = max_seq_length\n    num_layers: int = 5\n    dropout_rate: float = 0.2\n    use_lm_head: float = False\n    num_classes: int = None\n    vocab_size: int = None\n    final_activation = None\n    loss: Union[str, keras.losses.Loss] = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer: Union[str, keras.optimizers.Optimizer] = keras.optimizers.AdamW()\n    metrics = ['accuracy']\n\n    def __post_init__(self):\n        self.model_internal_dim: int = int(self.projection_expand_factor * self.model_input_dims)\n\n        self.delta_t_rank = math.ceil(self.model_input_dims/16)\n        if self.layer_id == -1:\n            self.layer_id = np.round(np.random.randint(0, 1000), 4)\n\n        if self.vocab_size == None:\n            raise ValueError(\"vocab size cannot be none\")\n\n        if self.use_lm_head:\n            self.num_classes=self.vocab_size\n        else:\n            if self.num_classes == None:\n                raise ValueError(f'num classes cannot be {self.num_classes}')\n\n            if self.num_classes == 1:\n                self.final_activation = 'sigmoid'\n            else:\n                self.final_activation = 'softmax'\n\n        if self.loss == None:\n            raise ValueError(f\"loss cannot be {self.loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.393903Z","iopub.execute_input":"2024-08-14T15:40:01.394321Z","iopub.status.idle":"2024-08-14T15:40:01.410639Z","shell.execute_reply.started":"2024-08-14T15:40:01.394291Z","shell.execute_reply":"2024-08-14T15:40:01.409743Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\nvocab_size = tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.412238Z","iopub.execute_input":"2024-08-14T15:40:01.412518Z","iopub.status.idle":"2024-08-14T15:40:01.580379Z","shell.execute_reply.started":"2024-08-14T15:40:01.412494Z","shell.execute_reply":"2024-08-14T15:40:01.579661Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def selective_scan(u, delta, A, B, C, D):\n    # first step of A_bar = exp(ΔA), i.e., ΔA\n    dA = tf.einsum('bld,dn->bldn', delta, A) \n    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)\n    \n    dA_cumsum = tf.pad(\n        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n    \n    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1\n    \n    # Cumulative sum along all the input tokens, parallel prefix sum, \n    # calculates dA for all the input tokens parallely\n    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  \n\n    # second step of A_bar = exp(ΔA), i.e., exp(ΔA)\n    dA_cumsum = tf.exp(dA_cumsum)  \n    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1\n\n    x = dB_u * dA_cumsum\n    # 1e-12 to avoid division by 0\n    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) \n\n    y = tf.einsum('bldn,bln->bld', x, C)\n    \n    return y + u * D ","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.581724Z","iopub.execute_input":"2024-08-14T15:40:01.582014Z","iopub.status.idle":"2024-08-14T15:40:01.589723Z","shell.execute_reply.started":"2024-08-14T15:40:01.581977Z","shell.execute_reply":"2024-08-14T15:40:01.588878Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class MambaBlock(layers.Layer):\n    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.args = modelargs\n        args = modelargs\n        self.layer_id = modelargs.layer_id\n\n        self.in_projection = layers.Dense(\n            args.model_internal_dim * 2, \n            input_shape=(args.model_input_dims,), use_bias=False)\n\n        self.conv1d = layers.Conv1D(\n            filters=args.model_internal_dim,\n            use_bias=args.conv_use_bias,\n            kernel_size=args.conv_kernel_size,\n            groups=args.model_internal_dim,\n            data_format='channels_first',\n            padding='causal'\n        )\n\n        # this layer takes in current token 'x' \n        # and outputs the input-specific Δ, B, C (according to S6)\n        self.x_projection = layers.Dense(args.delta_t_rank + args.model_states * 2, use_bias=False)\n\n        # this layer projects Δ from delta_t_rank to the mamba internal \n        # dimension\n        self.delta_t_projection = layers.Dense(args.model_internal_dim, \n                                               input_shape=(args.delta_t_rank,), use_bias=True)\n\n        self.A = repeat(\n                tf.range(1, args.model_states+1, dtype=tf.float32), \n                'n -> d n', d=args.model_internal_dim)\n\n        self.A_log = tf.Variable(\n                tf.math.log(self.A), \n                trainable=True, dtype=tf.float32, \n                name=f\"SSM_A_log_{args.layer_id}\")\n\n        self.D = tf.Variable(\n                np.ones(args.model_internal_dim), \n                trainable=True, dtype=tf.float32, \n                name=f\"SSM_D_{args.layer_id}\")\n\n        self.out_projection = layers.Dense(\n                args.model_input_dims, \n                input_shape=(args.model_internal_dim,), \n                use_bias=args.dense_use_bias)\n\n    def call(self, x):\n        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba pape.\n        Official Implementation:\n            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n        \"\"\"\n\n        (batch_size, seq_len, dimension) = x.shape\n\n        x_and_res = self.in_projection(x) # shape = (batch, seq_len, 2 * model_internal_dimension)\n        (x, res) = tf.split(x_and_res, \n                            [self.args.model_internal_dim, \n                             self.args.model_internal_dim], axis=-1)\n        \n        x = rearrange(x, 'b l d_in -> b d_in l')\n        x = self.conv1d(x)[:, :, :seq_len]\n        x = rearrange(x, 'b d_in l -> b l d_in')\n        \n        x = tf.nn.swish(x)\n        y = self.ssm(x)\n        y = y * tf.nn.swish(res)\n        return self.out_projection(y)\n    \n    def ssm(self, x):\n        \"\"\"Runs the SSM. See:\n            - Algorithm 2 in Section 3.2 in the Mamba paper\n            - run_SSM(A, B, C, u) in The Annotated S4\n            Official Implementation:\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n        \"\"\"\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n        #                                  and is why Mamba is called **selective** state spaces)\n\n        A = -tf.exp(tf.cast(self.A_log, tf.float32)) # shape -> (d_in, n)\n        D = tf.cast(self.D, tf.float32)\n\n        x_dbl = self.x_projection(x) # shape -> (batch, seq_len, delta_t_rank + 2*n)\n\n        (delta, B, C) = tf.split(\n                x_dbl, \n                num_or_size_splits=[self.args.delta_t_rank, n, n], \n                axis=-1) # delta.shape -> (batch, seq_len) & B, C shape -> (batch, seq_len, n)\n\n        delta = tf.nn.softplus(self.delta_t_projection(delta)) # shape -> (batch, seq_len, model_input_dim)\n\n        return selective_scan(x, delta, A, B, C, D)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.592206Z","iopub.execute_input":"2024-08-14T15:40:01.592550Z","iopub.status.idle":"2024-08-14T15:40:01.610799Z","shell.execute_reply.started":"2024-08-14T15:40:01.592519Z","shell.execute_reply":"2024-08-14T15:40:01.609985Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(layers.Layer):\n    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.args = modelargs\n        self.mixer = MambaBlock(modelargs)\n        self.norm = layers.LayerNormalization(epsilon=1e-5)\n\n    def call(self, x):\n        \"\"\"\n        Official Implementation:\n            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n            \n            Note: the official repo chains residual blocks that look like\n                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n            where the first Add is a no-op. This is purely for performance reasons as this\n            allows them to fuse the Add->Norm.\n\n            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n            \n        \"\"\"\n        return self.mixer(self.norm(x)) + x","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.611943Z","iopub.execute_input":"2024-08-14T15:40:01.612536Z","iopub.status.idle":"2024-08-14T15:40:01.622482Z","shell.execute_reply.started":"2024-08-14T15:40:01.612504Z","shell.execute_reply":"2024-08-14T15:40:01.621729Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def init_model(args: ModelArgs):\n    input_layer = layers.Input(shape=(args.seq_length,), name='input_ids')\n    x = layers.Embedding(args.vocab_size, args.model_input_dims, input_length=args.seq_length)(input_layer)\n\n    for i in range(args.num_layers):\n        x = ResidualBlock(args, name=f\"Residual_{i}\")(x)\n        x = layers.Dropout(args.dropout_rate)(x)\n\n    x = layers.LayerNormalization(epsilon=1e-5)(x)\n\n    if not args.use_lm_head: # use flatten only if we are using the model as an LM\n        x = layers.Flatten()(x)\n    x = layers.Dense(1024, activation=tf.nn.gelu)(x)\n    output_layer = layers.Dense(args.num_classes, activation=args.final_activation)(x)\n\n    model = Model(inputs=input_layer, outputs=output_layer, name='MambaTimeModel')\n    model.compile(\n        loss=args.loss,\n        optimizer=args.optimizer,\n        metrics=args.metrics\n    )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.623567Z","iopub.execute_input":"2024-08-14T15:40:01.623837Z","iopub.status.idle":"2024-08-14T15:40:01.634450Z","shell.execute_reply.started":"2024-08-14T15:40:01.623815Z","shell.execute_reply":"2024-08-14T15:40:01.633397Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"args = ModelArgs(\n    model_input_dims=max_seq_length,\n    model_states=32,\n    num_layers=12,\n    dropout_rate=0.2,\n    vocab_size=vocab_size,\n    num_classes=1,\n    loss='binary_crossentropy',\n)\nmodel = init_model(args)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:01.635549Z","iopub.execute_input":"2024-08-14T15:40:01.635828Z","iopub.status.idle":"2024-08-14T15:40:05.969533Z","shell.execute_reply.started":"2024-08-14T15:40:01.635805Z","shell.execute_reply":"2024-08-14T15:40:05.968628Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b99981f0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b99981f0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b99980d0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b99980d0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9b4d480> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9b4d480>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b999a0e0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b999a0e0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b999a710> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b999a710>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b999b910> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b999b910>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9a52050> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9a52050>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9a525f0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9a525f0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9a53be0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9a53be0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9911fc0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9911fc0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9911ea0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9911ea0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9913400> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9913400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b97d9f30> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b97d9f30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b97da290> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b97da290>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b97db760> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b97db760>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b96b9f30> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b96b9f30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b96ba200> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b96ba200>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b96bba30> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b96bba30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9589ea0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9589ea0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9589c60> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9589c60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b958b250> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b958b250>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b964dea0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b964dea0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b964e830> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b964e830>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b964f640> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b964f640>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b950de10> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b950de10>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b950e0e0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b950e0e0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b950f910> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b950f910>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b93e1cf0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b93e1cf0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b93e1bd0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b93e1bd0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b93e3130> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b93e3130>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b92a9c60> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b92a9c60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b92a9fc0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b92a9fc0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b92ab490> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b92ab490>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9175bd0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9175bd0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9176170> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9176170>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983b9175fc0> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983b9175fc0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"MambaTimeModel\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MambaTimeModel\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m25,730,048\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_0 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_1 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_2 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_3 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_4 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_5 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_6 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_7 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_8 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_9 (\u001b[38;5;33mResidualBlock\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_10 (\u001b[38;5;33mResidualBlock\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_11 (\u001b[38;5;33mResidualBlock\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,711,104\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ layer_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_98 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │   \u001b[38;5;34m268,436,480\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_99 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,025\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,730,048</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Residual_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,711,104</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ layer_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">268,436,480</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m314,701,825\u001b[0m (1.17 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">314,701,825</span> (1.17 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m314,701,825\u001b[0m (1.17 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">314,701,825</span> (1.17 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tqdm import tqdm\n\ndataset = load_dataset(\"neuralwork/fashion-style-instruct\", split=\"train\")\ndataset = dataset.train_test_split(test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:05.970591Z","iopub.execute_input":"2024-08-14T15:40:05.970856Z","iopub.status.idle":"2024-08-14T15:40:07.025332Z","shell.execute_reply.started":"2024-08-14T15:40:05.970831Z","shell.execute_reply":"2024-08-14T15:40:07.024469Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.027867Z","iopub.execute_input":"2024-08-14T15:40:07.028163Z","iopub.status.idle":"2024-08-14T15:40:07.033952Z","shell.execute_reply.started":"2024-08-14T15:40:07.028138Z","shell.execute_reply":"2024-08-14T15:40:07.033066Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'completion', 'context'],\n        num_rows: 2235\n    })\n    test: Dataset({\n        features: ['input', 'completion', 'context'],\n        num_rows: 958\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token \nEOS_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.035152Z","iopub.execute_input":"2024-08-14T15:40:07.035481Z","iopub.status.idle":"2024-08-14T15:40:07.043807Z","shell.execute_reply.started":"2024-08-14T15:40:07.035447Z","shell.execute_reply":"2024-08-14T15:40:07.043070Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}]},{"cell_type":"code","source":"alpacaFormatString = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Context:\n{}\n\n### Input:\n{}\n\n### Completion:\n{}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.046716Z","iopub.execute_input":"2024-08-14T15:40:07.047071Z","iopub.status.idle":"2024-08-14T15:40:07.051834Z","shell.execute_reply.started":"2024-08-14T15:40:07.047046Z","shell.execute_reply":"2024-08-14T15:40:07.050919Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Define the function to create the new 'text' column\ndef formatDatasetAlpaca(sample):\n    context = sample['context']\n    inputText = sample['input']\n    completion = sample['completion']\n    \n    text = alpacaFormatString.format(context, inputText, completion) + EOS_TOKEN\n    sample['text'] = text\n    \n    return sample\n\n# Apply the function to the dataset\ndataset = dataset.map(formatDatasetAlpaca)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.052988Z","iopub.execute_input":"2024-08-14T15:40:07.053372Z","iopub.status.idle":"2024-08-14T15:40:07.564417Z","shell.execute_reply.started":"2024-08-14T15:40:07.053346Z","shell.execute_reply":"2024-08-14T15:40:07.563484Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2235 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5ee723ec6784145ad1c2b33998c410e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/958 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e3217da3744445d8b162c650a5fd6a8"}},"metadata":{}}]},{"cell_type":"code","source":"import pprint\npprint.pprint(dataset['train'][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.565606Z","iopub.execute_input":"2024-08-14T15:40:07.565861Z","iopub.status.idle":"2024-08-14T15:40:07.574196Z","shell.execute_reply.started":"2024-08-14T15:40:07.565838Z","shell.execute_reply":"2024-08-14T15:40:07.573072Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"{'completion': 'Outfit Combination 1:\\n'\n               '- Top: A lightweight, short-sleeved button-up shirt in a solid '\n               'color like navy or olive green. Choose a fabric with a bit of '\n               'stretch for added comfort and flexibility.\\n'\n               '- Bottom: Pair the shirt with flat-front chino shorts in a '\n               'lighter shade, like beige or khaki. Opt for a mid-length that '\n               'hits just above the knee.\\n'\n               '- Shoes: Slip-on loafers in a neutral tone, such as brown or '\n               'tan. Look for a sleek design with minimal details.\\n'\n               '- Accessories: Finish the look with a woven belt in a '\n               'complementary color and a pair of polarized sunglasses for a '\n               'stylish touch.\\n'\n               '\\n'\n               'Outfit Combination 2:\\n'\n               '- Top: A slim-fit classic polo shirt in a vibrant color like '\n               'cobalt blue or emerald green. Look for a polo with a slightly '\n               \"longer back hem to ensure it doesn't ride up.\\n\"\n               '- Bottom: Opt for flat-front tailored shorts in a darker '\n               'shade, such as navy or charcoal gray. Choose a length that '\n               'falls just above the knee for a polished look.\\n'\n               '- Shoes: Go for a pair of versatile boat shoes in a '\n               'contrasting color, like a light brown or tan. Make sure they '\n               'fit well and provide good support.\\n'\n               '- Accessories: Add a braided leather bracelet for a touch of '\n               'casual elegance and a lightweight linen/cotton blend blazer in '\n               'a complementary color for a smart evening outfit option.\\n'\n               '\\n'\n               'Outfit Combination 3:\\n'\n               '- Top: A well-fitted short-sleeved linen shirt in a soft '\n               'pastel shade, like light pink or sky blue. Linen will keep you '\n               'cool and add a touch of sophistication.\\n'\n               '- Bottom: Choose tailored linen trousers in a slightly darker '\n               'color, such as taupe or light gray. Look for a relaxed fit to '\n               'create a breezy vibe.\\n'\n               '- Shoes: Opt for suede loafers in a complementary shade, like '\n               'beige or light brown. They add a refined touch to the outfit.\\n'\n               '- Accessories: Add a straw fedora hat for a tropical flair, '\n               'and wear a leather woven belt in a coordinating color to '\n               'complete the ensemble.\\n'\n               '\\n'\n               'Outfit Combination 4:\\n'\n               '- Top: A lightweight, well-fitted linen-blend short-sleeved '\n               'shirt in a subtle print, such as micro-floral or geometric '\n               'patterns. Stick to lighter tones like cream or light gray.\\n'\n               '- Bottom: Pair the shirt with tailored flat-front shorts in a '\n               'similar shade. Choose a comfortable length that hits just '\n               'above the knee.\\n'\n               '- Shoes: Go for a pair of neutral-colored espadrilles for a '\n               'vacation-ready look. The woven texture adds depth and style.\\n'\n               '- Accessories: Wear a beaded bracelet with wooden or volcanic '\n               'stone accents to add an earthy touch. Combine it with a pair '\n               'of sunglasses with colored or mirrored lenses to finish the '\n               'look.\\n'\n               '\\n'\n               'Outfit Combination 5:\\n'\n               '- Top: A classic short-sleeved linen button-up shirt in a bold '\n               'color like coral or turquoise. Opt for a slimmer fit to '\n               'accentuate your body shape.\\n'\n               '- Bottom: Pair the shirt with tailored shorts in a contrasting '\n               'color, such as navy or charcoal gray. Look for a length that '\n               'falls just above the knee.\\n'\n               '- Shoes: Choose a pair of leather sandals in a comfortable, '\n               'supportive design. Look for a style that can be easily dressed '\n               'up or down.\\n'\n               '- Accessories: Add a woven beach-inspired belt with a bit of '\n               'rope detailing for a nautical touch. Complete the look with a '\n               'straw Panama hat for a touch of elegance and sun protection.',\n 'context': \"I'm going to a tropical vacation.\",\n 'input': \"I'm a tall man with an endomorph body type, carrying more weight \"\n          'around my midsection. I prefer structured, well-tailored clothes '\n          'that provide a streamlined look.',\n 'text': 'Below is an instruction that describes a task, paired with an input '\n         'that provides further context. Write a response that appropriately '\n         'completes the request.\\n'\n         '\\n'\n         '### Context:\\n'\n         \"I'm going to a tropical vacation.\\n\"\n         '\\n'\n         '### Input:\\n'\n         \"I'm a tall man with an endomorph body type, carrying more weight \"\n         'around my midsection. I prefer structured, well-tailored clothes '\n         'that provide a streamlined look.\\n'\n         '\\n'\n         '### Completion:\\n'\n         'Outfit Combination 1:\\n'\n         '- Top: A lightweight, short-sleeved button-up shirt in a solid color '\n         'like navy or olive green. Choose a fabric with a bit of stretch for '\n         'added comfort and flexibility.\\n'\n         '- Bottom: Pair the shirt with flat-front chino shorts in a lighter '\n         'shade, like beige or khaki. Opt for a mid-length that hits just '\n         'above the knee.\\n'\n         '- Shoes: Slip-on loafers in a neutral tone, such as brown or tan. '\n         'Look for a sleek design with minimal details.\\n'\n         '- Accessories: Finish the look with a woven belt in a complementary '\n         'color and a pair of polarized sunglasses for a stylish touch.\\n'\n         '\\n'\n         'Outfit Combination 2:\\n'\n         '- Top: A slim-fit classic polo shirt in a vibrant color like cobalt '\n         'blue or emerald green. Look for a polo with a slightly longer back '\n         \"hem to ensure it doesn't ride up.\\n\"\n         '- Bottom: Opt for flat-front tailored shorts in a darker shade, such '\n         'as navy or charcoal gray. Choose a length that falls just above the '\n         'knee for a polished look.\\n'\n         '- Shoes: Go for a pair of versatile boat shoes in a contrasting '\n         'color, like a light brown or tan. Make sure they fit well and '\n         'provide good support.\\n'\n         '- Accessories: Add a braided leather bracelet for a touch of casual '\n         'elegance and a lightweight linen/cotton blend blazer in a '\n         'complementary color for a smart evening outfit option.\\n'\n         '\\n'\n         'Outfit Combination 3:\\n'\n         '- Top: A well-fitted short-sleeved linen shirt in a soft pastel '\n         'shade, like light pink or sky blue. Linen will keep you cool and add '\n         'a touch of sophistication.\\n'\n         '- Bottom: Choose tailored linen trousers in a slightly darker color, '\n         'such as taupe or light gray. Look for a relaxed fit to create a '\n         'breezy vibe.\\n'\n         '- Shoes: Opt for suede loafers in a complementary shade, like beige '\n         'or light brown. They add a refined touch to the outfit.\\n'\n         '- Accessories: Add a straw fedora hat for a tropical flair, and wear '\n         'a leather woven belt in a coordinating color to complete the '\n         'ensemble.\\n'\n         '\\n'\n         'Outfit Combination 4:\\n'\n         '- Top: A lightweight, well-fitted linen-blend short-sleeved shirt in '\n         'a subtle print, such as micro-floral or geometric patterns. Stick to '\n         'lighter tones like cream or light gray.\\n'\n         '- Bottom: Pair the shirt with tailored flat-front shorts in a '\n         'similar shade. Choose a comfortable length that hits just above the '\n         'knee.\\n'\n         '- Shoes: Go for a pair of neutral-colored espadrilles for a '\n         'vacation-ready look. The woven texture adds depth and style.\\n'\n         '- Accessories: Wear a beaded bracelet with wooden or volcanic stone '\n         'accents to add an earthy touch. Combine it with a pair of sunglasses '\n         'with colored or mirrored lenses to finish the look.\\n'\n         '\\n'\n         'Outfit Combination 5:\\n'\n         '- Top: A classic short-sleeved linen button-up shirt in a bold color '\n         'like coral or turquoise. Opt for a slimmer fit to accentuate your '\n         'body shape.\\n'\n         '- Bottom: Pair the shirt with tailored shorts in a contrasting '\n         'color, such as navy or charcoal gray. Look for a length that falls '\n         'just above the knee.\\n'\n         '- Shoes: Choose a pair of leather sandals in a comfortable, '\n         'supportive design. Look for a style that can be easily dressed up or '\n         'down.\\n'\n         '- Accessories: Add a woven beach-inspired belt with a bit of rope '\n         'detailing for a nautical touch. Complete the look with a straw '\n         'Panama hat for a touch of elegance and sun protection.<|endoftext|>'}\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.575711Z","iopub.execute_input":"2024-08-14T15:40:07.575990Z","iopub.status.idle":"2024-08-14T15:40:07.608324Z","shell.execute_reply.started":"2024-08-14T15:40:07.575965Z","shell.execute_reply":"2024-08-14T15:40:07.607413Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'completion', 'context', 'text'],\n        num_rows: 2235\n    })\n    test: Dataset({\n        features: ['input', 'completion', 'context', 'text'],\n        num_rows: 958\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Extract the \"text\" column for train and test datasets\ntrain_texts = dataset['train']['text']\ntest_texts = dataset['test']['text']","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.609432Z","iopub.execute_input":"2024-08-14T15:40:07.609683Z","iopub.status.idle":"2024-08-14T15:40:07.625043Z","shell.execute_reply.started":"2024-08-14T15:40:07.609659Z","shell.execute_reply":"2024-08-14T15:40:07.624271Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"len(train_texts)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:46:29.621484Z","iopub.execute_input":"2024-08-14T15:46:29.622208Z","iopub.status.idle":"2024-08-14T15:46:29.628444Z","shell.execute_reply.started":"2024-08-14T15:46:29.622174Z","shell.execute_reply":"2024-08-14T15:46:29.627586Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"2235"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize the texts\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_seq_length)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:07.626053Z","iopub.execute_input":"2024-08-14T15:40:07.626302Z","iopub.status.idle":"2024-08-14T15:40:10.083831Z","shell.execute_reply.started":"2024-08-14T15:40:07.626273Z","shell.execute_reply":"2024-08-14T15:40:10.082798Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_encodings.keys()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:48:03.466635Z","iopub.execute_input":"2024-08-14T15:48:03.467031Z","iopub.status.idle":"2024-08-14T15:48:03.474048Z","shell.execute_reply.started":"2024-08-14T15:48:03.466978Z","shell.execute_reply":"2024-08-14T15:48:03.472868Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'attention_mask'])"},"metadata":{}}]},{"cell_type":"code","source":"train_encodings['labels'] = train_encodings['input_ids']\ntest_encodings['labels'] = test_encodings['input_ids']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert lists to tensors\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings)\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings)\n))\n\n# Batch and shuffle the datasets\nBATCH_SIZE = 32\ntrain_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:40:10.085366Z","iopub.execute_input":"2024-08-14T15:40:10.085649Z","iopub.status.idle":"2024-08-14T15:40:19.619946Z","shell.execute_reply.started":"2024-08-14T15:40:10.085624Z","shell.execute_reply":"2024-08-14T15:40:19.619171Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset, epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T15:45:45.927353Z","iopub.execute_input":"2024-08-14T15:45:45.927716Z","iopub.status.idle":"2024-08-14T15:45:47.498771Z","shell.execute_reply.started":"2024-08-14T15:45:45.927687Z","shell.execute_reply":"2024-08-14T15:45:47.497679Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba616320> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba616320>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba970550> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba970550>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba971990> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba971990>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba973b50> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba973b50>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba9d1d80> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba9d1d80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983baa48040> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983baa48040>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983baa4a170> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983baa4a170>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba7c4430> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba7c4430>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba7c6560> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba7c6560>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba83c790> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba83c790>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba83eb00> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba83eb00>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function conv.<locals>._conv_xla at 0x7983ba4ad090> and will run it as-is.\nCause: Unable to locate the source code of <function conv.<locals>._conv_xla at 0x7983ba4ad090>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     arguments_context\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  • \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arguments_context:\n\u001b[0;32m--> 122\u001b[0m     arguments_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arguments_context)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# Get original error message and append information to it.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOpError):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optree/ops.py:747\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    745\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    746\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: None values not supported."],"ename":"ValueError","evalue":"None values not supported.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}