{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOioZKl34c1k",
        "outputId": "246ba76a-0344-443d-9df3-fb7df2b9587c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug  1 12:31:37 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU einops dataclasses typing datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AQQ4kF6b0fD",
        "outputId": "98296248-2669-4153-a0e7-b50bfb325aba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "from typing import Union\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_scheduler, AutoTokenizer, MambaForCausalLM\n",
        "from datasets import load_dataset\n",
        "import pprint as pp"
      ],
      "metadata": {
        "id": "gAVXea-zcSXU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createPrompts(sample):\n",
        "    \"\"\"\n",
        "    Reads our dataset, creates a column of prompts corresponding\n",
        "    to each row, and returns a list of all the prompts.\n",
        "    \"\"\"\n",
        "\n",
        "    formatString = \"\"\"### Context:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Completion:\n",
        "{}\"\"\"\n",
        "\n",
        "    context = sample['context']\n",
        "    input = sample['input']\n",
        "    completion = sample['completion']\n",
        "\n",
        "    prompt = formatString.format(context, input, completion)\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt\n",
        "    }"
      ],
      "metadata": {
        "id": "c1lAv7mW8ISS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadStringsFromDataset(dataset):\n",
        "    dataset = dataset.map(createPrompts)\n",
        "\n",
        "    listOfStrings = []\n",
        "    for sample in dataset:\n",
        "        listOfStrings.append(sample['prompt'])\n",
        "\n",
        "    return listOfStrings"
      ],
      "metadata": {
        "id": "EKWcfm3h-P8R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, context_len=384):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # Load and tokenize data\n",
        "        self.data = loadStringsFromDataset(dataset)\n",
        "\n",
        "        self.tokens = tokenizer(self.data, return_tensors='pt', truncation=True, padding='max_length', max_length=context_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokens['input_ids'][idx],\n",
        "            'labels': self.tokens['input_ids'][idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "wH9jWOpNf8mw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    vocab_size: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_conv: int = 4\n",
        "    pad_vocab_size_multiple: int = 8\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
        "            self.vocab_size += (self.pad_vocab_size_multiple - self.vocab_size % self.pad_vocab_size_multiple)"
      ],
      "metadata": {
        "id": "dkJh5veEcomD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "UD7K2LbGjCwa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.mixer = MambaBlock(args)\n",
        "        self.norm = RMSNorm(args.d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.mixer(self.norm(x)) + x\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "IhO-hsHLjDpq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=args.d_inner,\n",
        "            out_channels=args.d_inner,\n",
        "            bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv,\n",
        "            groups=args.d_inner,\n",
        "            padding=args.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
        "\n",
        "        # dt_proj projects Δ from dt_rank to d_in\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        (b, l, d) = x.shape\n",
        "\n",
        "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
        "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "\n",
        "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
        "        x = self.conv1d(x)[:, :, :l]\n",
        "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
        "\n",
        "        x = F.silu(x)\n",
        "\n",
        "        y = self.ssm(x)\n",
        "\n",
        "        y = y * F.silu(res)\n",
        "\n",
        "        output = self.out_proj(y)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def ssm(self, x):\n",
        "\n",
        "        (d_in, n) = self.A_log.shape\n",
        "\n",
        "        # Compute ∆ A B C D, the state space parameters.\n",
        "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
        "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
        "        #                                  and is why Mamba is called **selective** state spaces)\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
        "        D = self.D.float()\n",
        "\n",
        "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
        "\n",
        "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
        "\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D):\n",
        "\n",
        "        (b, l, d_in) = u.shape\n",
        "        n = A.shape[1]\n",
        "\n",
        "        # Discretize continuous parameters (A, B)\n",
        "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
        "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
        "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
        "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n",
        "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "\n",
        "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
        "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n",
        "        # is additionally hardware-aware (like FlashAttention).\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "\n",
        "        y = y + u * D\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "PWMCK-nZjsyI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mamba(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        \"\"\"Full Mamba model.\"\"\"\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
        "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
        "        self.norm_f = RMSNorm(args.d_model)\n",
        "\n",
        "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
        "                                                     # See \"Weight Tying\" paper\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            nn.init.ones_(module.weight)\n",
        "        elif isinstance(module, nn.MultiheadAttention):\n",
        "            nn.init.xavier_uniform_(module.in_proj_weight)\n",
        "            if module.in_proj_bias is not None:\n",
        "                nn.init.zeros_(module.in_proj_bias)\n",
        "            nn.init.xavier_uniform_(module.out_proj.weight)\n",
        "            if module.out_proj.bias is not None:\n",
        "                nn.init.zeros_(module.out_proj.bias)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pretrained(pretrained_model_name: str):\n",
        "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
        "\n",
        "                * 'state-spaces/mamba-2.8b-slimpj'\n",
        "                * 'state-spaces/mamba-2.8b'\n",
        "                * 'state-spaces/mamba-1.4b'\n",
        "                * 'state-spaces/mamba-790m'\n",
        "                * 'state-spaces/mamba-370m'\n",
        "                * 'state-spaces/mamba-130m'\n",
        "\n",
        "        \"\"\"\n",
        "        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
        "        from transformers.utils.hub import cached_file\n",
        "\n",
        "        def load_config_hf(model_name):\n",
        "            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n",
        "                                                _raise_exceptions_for_missing_entries=False)\n",
        "            return json.load(open(resolved_archive_file))\n",
        "\n",
        "\n",
        "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
        "            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n",
        "                                                _raise_exceptions_for_missing_entries=False)\n",
        "            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n",
        "\n",
        "        config_data = load_config_hf(pretrained_model_name)\n",
        "        args = ModelArgs(\n",
        "            d_model=config_data['d_model'],\n",
        "            n_layer=config_data['n_layer'],\n",
        "            vocab_size=config_data['vocab_size']\n",
        "        )\n",
        "        model = Mamba(args)\n",
        "\n",
        "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
        "        new_state_dict = {}\n",
        "        for key in state_dict:\n",
        "            new_key = key.replace('backbone.', '')\n",
        "            new_state_dict[new_key] = state_dict[key]\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "Jei2l5_Kpb-P"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = ModelArgs(\n",
        "    d_model=768,            # Hidden dimension size\n",
        "    n_layer=24,             # Number of layers\n",
        "    vocab_size=50280,       # Vocabulary size\n",
        "    d_state=3072,           # Latent state dimension\n",
        "    expand=4,             # Expansion factor\n",
        "    dt_rank='auto',       # Rank of delta\n",
        "    d_conv=4,             # Convolution kernel size\n",
        "    pad_vocab_size_multiple=8,\n",
        "    conv_bias=True,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "model = Mamba(args)"
      ],
      "metadata": {
        "id": "GIIrq-csp3UJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Mamba.from_pretrained('state-spaces/mamba-130m')\n",
        "tokenizer = AutoTokenizer.from_pretrained('state-spaces/mamba-130m-hf')"
      ],
      "metadata": {
        "id": "4tPusBL7rFOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33852ac-0fb0-41b6-9782-6dcc28568389"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"neuralwork/fashion-style-instruct\", split='train')\n",
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "WzifbFCxt1Dx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    # you can change it to match your setup\n",
        "    trainDataset = dataset['train']\n",
        "    testDataset = dataset['test']\n",
        "    lr = 1e-4\n",
        "    epochs = 100\n",
        "    context_len = 384\n",
        "    train_batch_size = 2\n",
        "    valid_batch_size = 2\n",
        "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "OP3RiiAsrFzj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{Args.trainDataset}\\n\\n{Args.testDataset}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynZr8zYq7BK9",
        "outputId": "fcca7ef8-24d0-4671-c71c-63bc9eae647d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input', 'completion', 'context'],\n",
            "    num_rows: 800\n",
            "})\n",
            "\n",
            "Dataset({\n",
            "    features: ['input', 'completion', 'context'],\n",
            "    num_rows: 200\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(Args.trainDataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "2yyTJuur78EY",
        "outputId": "336e1535-f355-4b54-dfae-f21a780e4908"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>datasets.arrow_dataset.Dataset</b><br/>def __init__(arrow_table: Table, info: Optional[DatasetInfo]=None, split: Optional[NamedSplit]=None, indices_table: Optional[Table]=None, fingerprint: Optional[str]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py</a>A Dataset backed by an Arrow table.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 668);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_dataset = TextDataset(Args.trainDataset, tokenizer, context_len=Args.context_len)\n",
        "eval_dataset = TextDataset(Args.testDataset, tokenizer, context_len=Args.context_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=Args.train_batch_size, shuffle=False)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=Args.valid_batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=Args.lr)\n",
        "scheduler = get_scheduler(\n",
        "    \"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader) * Args.epochs\n",
        ")"
      ],
      "metadata": {
        "id": "GlK6PJ-N6u6c"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(Args.device)\n",
        "for epoch in range(Args.epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(Args.device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(batch['input_ids'])\n",
        "\n",
        "        # Compute the loss manually\n",
        "        shift_logits = outputs[..., :-1, :].contiguous()\n",
        "        shift_labels = batch['labels'][..., 1:].contiguous()\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{Args.epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            batch = {k: v.to(Args.device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(batch['input_ids'])\n",
        "\n",
        "            # Compute the loss manually\n",
        "            shift_logits = outputs[..., :-1, :].contiguous()\n",
        "            shift_labels = batch['labels'][..., 1:].contiguous()\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{Args.epochs}, Evaluation Loss: {avg_eval_loss}\")\n",
        "    model_save_path = \"mamba_darija.pt\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "yZDqNqCtwyJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQfK9nsp-56D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}