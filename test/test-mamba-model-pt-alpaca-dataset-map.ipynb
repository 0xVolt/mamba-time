{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nOioZKl34c1k","outputId":"246ba76a-0344-443d-9df3-fb7df2b9587c","execution":{"iopub.status.busy":"2024-08-05T12:21:06.978204Z","iopub.execute_input":"2024-08-05T12:21:06.978697Z","iopub.status.idle":"2024-08-05T12:21:08.139826Z","shell.execute_reply.started":"2024-08-05T12:21:06.978658Z","shell.execute_reply":"2024-08-05T12:21:08.138337Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/bin/bash: nvidia-smi: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -qU einops dataclasses typing datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AQQ4kF6b0fD","outputId":"98296248-2669-4153-a0e7-b50bfb325aba","execution":{"iopub.status.busy":"2024-08-05T12:21:10.105396Z","iopub.execute_input":"2024-08-05T12:21:10.105864Z","iopub.status.idle":"2024-08-05T12:21:30.462850Z","shell.execute_reply.started":"2024-08-05T12:21:10.105825Z","shell.execute_reply":"2024-08-05T12:21:30.461204Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from __future__ import annotations\nimport math\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dataclasses import dataclass\nfrom einops import rearrange, repeat, einsum\nfrom typing import Union\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import get_scheduler, AutoTokenizer, MambaForCausalLM\nfrom datasets import load_dataset\nimport pprint as pp","metadata":{"id":"gAVXea-zcSXU","execution":{"iopub.status.busy":"2024-08-05T12:21:30.465637Z","iopub.execute_input":"2024-08-05T12:21:30.466049Z","iopub.status.idle":"2024-08-05T12:21:35.527608Z","shell.execute_reply.started":"2024-08-05T12:21:30.466012Z","shell.execute_reply":"2024-08-05T12:21:35.526566Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def createPrompts(sample):\n    \"\"\"\n    Reads our dataset, creates a column of prompts corresponding\n    to each row, and returns a list of all the prompts.\n    \"\"\"\n\n    formatString = \"\"\"### Context:\n{}\n\n### Input:\n{}\n\n### Completion:\n{}\"\"\"\n\n    context = sample['context']\n    input = sample['input']\n    completion = sample['completion']\n\n    prompt = formatString.format(context, input, completion)\n\n    return {\n        \"prompt\": prompt\n    }","metadata":{"id":"c1lAv7mW8ISS","execution":{"iopub.status.busy":"2024-08-05T12:21:35.528988Z","iopub.execute_input":"2024-08-05T12:21:35.529541Z","iopub.status.idle":"2024-08-05T12:21:35.536821Z","shell.execute_reply.started":"2024-08-05T12:21:35.529506Z","shell.execute_reply":"2024-08-05T12:21:35.535394Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def loadStringsFromDataset(dataset):\n    dataset = dataset.map(createPrompts)\n\n    listOfStrings = []\n    for sample in dataset:\n        listOfStrings.append(sample['prompt'])\n\n    return listOfStrings","metadata":{"id":"EKWcfm3h-P8R","execution":{"iopub.status.busy":"2024-08-05T12:21:35.539386Z","iopub.execute_input":"2024-08-05T12:21:35.539763Z","iopub.status.idle":"2024-08-05T12:21:35.557677Z","shell.execute_reply.started":"2024-08-05T12:21:35.539733Z","shell.execute_reply":"2024-08-05T12:21:35.556088Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Custom Dataset Class\nclass TextDataset(Dataset):\n    def __init__(self, dataset, tokenizer, context_len=384):\n        self.tokenizer = tokenizer\n        self.context_len = context_len\n\n        # Load and tokenize data\n        self.data = loadStringsFromDataset(dataset)\n\n        self.tokens = tokenizer(self.data, return_tensors='pt', truncation=True, padding='max_length', max_length=context_len)\n\n    def __len__(self):\n        return len(self.tokens['input_ids'])\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.tokens['input_ids'][idx],\n            'labels': self.tokens['input_ids'][idx]\n        }","metadata":{"id":"wH9jWOpNf8mw","execution":{"iopub.status.busy":"2024-08-05T12:21:35.559389Z","iopub.execute_input":"2024-08-05T12:21:35.559854Z","iopub.status.idle":"2024-08-05T12:21:35.572425Z","shell.execute_reply.started":"2024-08-05T12:21:35.559812Z","shell.execute_reply":"2024-08-05T12:21:35.570992Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelArgs:\n    d_model: int\n    n_layer: int\n    vocab_size: int\n    d_state: int = 16\n    expand: int = 2\n    dt_rank: Union[int, str] = 'auto'\n    d_conv: int = 4\n    pad_vocab_size_multiple: int = 8\n    conv_bias: bool = True\n    bias: bool = False\n\n    def __post_init__(self):\n        self.d_inner = int(self.expand * self.d_model)\n\n        if self.dt_rank == 'auto':\n            self.dt_rank = math.ceil(self.d_model / 16)\n\n        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n            self.vocab_size += (self.pad_vocab_size_multiple - self.vocab_size % self.pad_vocab_size_multiple)","metadata":{"id":"dkJh5veEcomD","execution":{"iopub.status.busy":"2024-08-05T12:21:35.573598Z","iopub.execute_input":"2024-08-05T12:21:35.573974Z","iopub.status.idle":"2024-08-05T12:21:35.588529Z","shell.execute_reply.started":"2024-08-05T12:21:35.573932Z","shell.execute_reply":"2024-08-05T12:21:35.587250Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self,\n                 d_model: int,\n                 eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n\n\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n\n        return output","metadata":{"id":"UD7K2LbGjCwa","execution":{"iopub.status.busy":"2024-08-05T12:21:35.590070Z","iopub.execute_input":"2024-08-05T12:21:35.590470Z","iopub.status.idle":"2024-08-05T12:21:35.606329Z","shell.execute_reply.started":"2024-08-05T12:21:35.590439Z","shell.execute_reply":"2024-08-05T12:21:35.605102Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n        super().__init__()\n        self.args = args\n        self.mixer = MambaBlock(args)\n        self.norm = RMSNorm(args.d_model)\n\n\n    def forward(self, x):\n        output = self.mixer(self.norm(x)) + x\n\n        return output","metadata":{"id":"IhO-hsHLjDpq","execution":{"iopub.status.busy":"2024-08-05T12:21:35.607673Z","iopub.execute_input":"2024-08-05T12:21:35.608023Z","iopub.status.idle":"2024-08-05T12:21:35.620910Z","shell.execute_reply.started":"2024-08-05T12:21:35.607994Z","shell.execute_reply":"2024-08-05T12:21:35.619477Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MambaBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n\n        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n\n        self.conv1d = nn.Conv1d(\n            in_channels=args.d_inner,\n            out_channels=args.d_inner,\n            bias=args.conv_bias,\n            kernel_size=args.d_conv,\n            groups=args.d_inner,\n            padding=args.d_conv - 1,\n        )\n\n        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n\n        # dt_proj projects Δ from dt_rank to d_in\n        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n\n        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n        self.A_log = nn.Parameter(torch.log(A))\n        self.D = nn.Parameter(torch.ones(args.d_inner))\n        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n\n\n    def forward(self, x):\n\n        (b, l, d) = x.shape\n\n        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n\n        x = rearrange(x, 'b l d_in -> b d_in l')\n        x = self.conv1d(x)[:, :, :l]\n        x = rearrange(x, 'b d_in l -> b l d_in')\n\n        x = F.silu(x)\n\n        y = self.ssm(x)\n\n        y = y * F.silu(res)\n\n        output = self.out_proj(y)\n\n        return output\n\n\n    def ssm(self, x):\n\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n        #                                  and is why Mamba is called **selective** state spaces)\n\n        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n        D = self.D.float()\n\n        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n\n        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n\n        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n\n        return y\n\n\n    def selective_scan(self, u, delta, A, B, C, D):\n\n        (b, l, d_in) = u.shape\n        n = A.shape[1]\n\n        # Discretize continuous parameters (A, B)\n        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n\n        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n        # is additionally hardware-aware (like FlashAttention).\n        x = torch.zeros((b, d_in, n), device=deltaA.device)\n        ys = []\n        for i in range(l):\n            x = deltaA[:, i] * x + deltaB_u[:, i]\n            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n            ys.append(y)\n        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n\n        y = y + u * D\n\n        return y","metadata":{"id":"PWMCK-nZjsyI","execution":{"iopub.status.busy":"2024-08-05T12:21:35.622725Z","iopub.execute_input":"2024-08-05T12:21:35.623161Z","iopub.status.idle":"2024-08-05T12:21:35.651941Z","shell.execute_reply.started":"2024-08-05T12:21:35.623128Z","shell.execute_reply":"2024-08-05T12:21:35.650630Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Mamba(nn.Module):\n    def __init__(self, args: ModelArgs):\n        \"\"\"Full Mamba model.\"\"\"\n        super().__init__()\n        self.args = args\n\n        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n        self.norm_f = RMSNorm(args.d_model)\n\n        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n        self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n                                                     # See \"Weight Tying\" paper\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        elif isinstance(module, RMSNorm):\n            nn.init.ones_(module.weight)\n        elif isinstance(module, nn.MultiheadAttention):\n            nn.init.xavier_uniform_(module.in_proj_weight)\n            if module.in_proj_bias is not None:\n                nn.init.zeros_(module.in_proj_bias)\n            nn.init.xavier_uniform_(module.out_proj.weight)\n            if module.out_proj.bias is not None:\n                nn.init.zeros_(module.out_proj.bias)\n\n\n    def forward(self, input_ids):\n\n        x = self.embedding(input_ids)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm_f(x)\n        logits = self.lm_head(x)\n\n        return logits\n\n\n    @staticmethod\n    def from_pretrained(pretrained_model_name: str):\n        \"\"\"Load pretrained weights from HuggingFace into model.\n\n                * 'state-spaces/mamba-2.8b-slimpj'\n                * 'state-spaces/mamba-2.8b'\n                * 'state-spaces/mamba-1.4b'\n                * 'state-spaces/mamba-790m'\n                * 'state-spaces/mamba-370m'\n                * 'state-spaces/mamba-130m'\n\n        \"\"\"\n        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n        from transformers.utils.hub import cached_file\n\n        def load_config_hf(model_name):\n            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n                                                _raise_exceptions_for_missing_entries=False)\n            return json.load(open(resolved_archive_file))\n\n\n        def load_state_dict_hf(model_name, device=None, dtype=None):\n            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n                                                _raise_exceptions_for_missing_entries=False)\n            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n\n        config_data = load_config_hf(pretrained_model_name)\n        args = ModelArgs(\n            d_model=config_data['d_model'],\n            n_layer=config_data['n_layer'],\n            vocab_size=config_data['vocab_size']\n        )\n        model = Mamba(args)\n\n        state_dict = load_state_dict_hf(pretrained_model_name)\n        new_state_dict = {}\n        for key in state_dict:\n            new_key = key.replace('backbone.', '')\n            new_state_dict[new_key] = state_dict[key]\n        model.load_state_dict(new_state_dict)\n\n        return model","metadata":{"id":"Jei2l5_Kpb-P","execution":{"iopub.status.busy":"2024-08-05T12:22:54.914657Z","iopub.execute_input":"2024-08-05T12:22:54.916206Z","iopub.status.idle":"2024-08-05T12:22:54.946038Z","shell.execute_reply.started":"2024-08-05T12:22:54.916138Z","shell.execute_reply":"2024-08-05T12:22:54.944068Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"args = ModelArgs(\n    d_model=768,            # Hidden dimension size\n    n_layer=24,             # Number of layers\n    vocab_size=50280,       # Vocabulary size\n    d_state=3072,           # Latent state dimension\n    expand=4,             # Expansion factor\n    dt_rank='auto',       # Rank of delta\n    d_conv=4,             # Convolution kernel size\n    pad_vocab_size_multiple=8,\n    conv_bias=True,\n    bias=False\n)\n\nmodel = Mamba(args)","metadata":{"id":"GIIrq-csp3UJ","execution":{"iopub.status.busy":"2024-08-05T12:22:57.932027Z","iopub.execute_input":"2024-08-05T12:22:57.932511Z","iopub.status.idle":"2024-08-05T12:23:14.263141Z","shell.execute_reply.started":"2024-08-05T12:22:57.932478Z","shell.execute_reply":"2024-08-05T12:23:14.261774Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-05T12:23:14.265119Z","iopub.execute_input":"2024-08-05T12:23:14.265662Z","iopub.status.idle":"2024-08-05T12:23:14.275993Z","shell.execute_reply.started":"2024-08-05T12:23:14.265629Z","shell.execute_reply":"2024-08-05T12:23:14.274777Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Mamba(\n  (embedding): Embedding(50280, 768)\n  (layers): ModuleList(\n    (0-23): 24 x ResidualBlock(\n      (mixer): MambaBlock(\n        (in_proj): Linear(in_features=768, out_features=6144, bias=False)\n        (conv1d): Conv1d(3072, 3072, kernel_size=(4,), stride=(1,), padding=(3,), groups=3072)\n        (x_proj): Linear(in_features=3072, out_features=6192, bias=False)\n        (dt_proj): Linear(in_features=48, out_features=3072, bias=True)\n        (out_proj): Linear(in_features=3072, out_features=768, bias=False)\n      )\n      (norm): RMSNorm()\n    )\n  )\n  (norm_f): RMSNorm()\n  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model = Mamba.from_pretrained('state-spaces/mamba-130m')\ntokenizer = AutoTokenizer.from_pretrained('state-spaces/mamba-130m-hf')","metadata":{"id":"4tPusBL7rFOI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f33852ac-0fb0-41b6-9782-6dcc28568389","execution":{"iopub.status.busy":"2024-08-05T12:23:45.500834Z","iopub.execute_input":"2024-08-05T12:23:45.501266Z","iopub.status.idle":"2024-08-05T12:23:52.913953Z","shell.execute_reply.started":"2024-08-05T12:23:45.501235Z","shell.execute_reply":"2024-08-05T12:23:52.912712Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f051660097a8421a9bd32d8a4101be1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/517M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a3ad5c04df424698f8a792ebcbf3ea"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee59d6d2146043e68b3480e9c7f9cdbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ecb61aabb4b4c4ab67c103e2e4c790c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-05T12:23:55.520921Z","iopub.execute_input":"2024-08-05T12:23:55.522147Z","iopub.status.idle":"2024-08-05T12:23:55.532452Z","shell.execute_reply.started":"2024-08-05T12:23:55.522099Z","shell.execute_reply":"2024-08-05T12:23:55.531058Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Mamba(\n  (embedding): Embedding(50280, 768)\n  (layers): ModuleList(\n    (0-23): 24 x ResidualBlock(\n      (mixer): MambaBlock(\n        (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n        (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n        (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n        (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n      )\n      (norm): RMSNorm()\n    )\n  )\n  (norm_f): RMSNorm()\n  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(\"neuralwork/fashion-style-instruct\", split='train')\ndataset = dataset.train_test_split(test_size=0.2)","metadata":{"id":"WzifbFCxt1Dx","execution":{"iopub.status.busy":"2024-08-05T12:24:33.848713Z","iopub.execute_input":"2024-08-05T12:24:33.849302Z","iopub.status.idle":"2024-08-05T12:24:38.163719Z","shell.execute_reply.started":"2024-08-05T12:24:33.849260Z","shell.execute_reply":"2024-08-05T12:24:38.161729Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/882 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31667c557f86427fac36e283c3c041dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.64M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959bd9946fa044d3a1f0093f1b674ea9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3193 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1d6ba343b24702b970c0c184ace8e7"}},"metadata":{}}]},{"cell_type":"code","source":"class Args:\n    # you can change it to match your setup\n    trainDataset = dataset['train']\n    testDataset = dataset['test']\n    lr = 1e-4\n    epochs = 1\n    context_len = 384\n    train_batch_size = 2\n    valid_batch_size = 2\n    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    device = torch.device(\"cpu\")","metadata":{"id":"OP3RiiAsrFzj","execution":{"iopub.status.busy":"2024-08-05T12:27:15.110074Z","iopub.execute_input":"2024-08-05T12:27:15.110531Z","iopub.status.idle":"2024-08-05T12:27:15.118120Z","shell.execute_reply.started":"2024-08-05T12:27:15.110497Z","shell.execute_reply":"2024-08-05T12:27:15.116094Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print(f'{Args.trainDataset}\\n\\n{Args.testDataset}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynZr8zYq7BK9","outputId":"fcca7ef8-24d0-4671-c71c-63bc9eae647d","execution":{"iopub.status.busy":"2024-08-05T12:27:15.522460Z","iopub.execute_input":"2024-08-05T12:27:15.522912Z","iopub.status.idle":"2024-08-05T12:27:15.529430Z","shell.execute_reply.started":"2024-08-05T12:27:15.522849Z","shell.execute_reply":"2024-08-05T12:27:15.528164Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input', 'completion', 'context'],\n    num_rows: 2554\n})\n\nDataset({\n    features: ['input', 'completion', 'context'],\n    num_rows: 639\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"type(Args.trainDataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"id":"2yyTJuur78EY","outputId":"336e1535-f355-4b54-dfae-f21a780e4908","execution":{"iopub.status.busy":"2024-08-05T12:27:15.955730Z","iopub.execute_input":"2024-08-05T12:27:15.956259Z","iopub.status.idle":"2024-08-05T12:27:15.966375Z","shell.execute_reply.started":"2024-08-05T12:27:15.956220Z","shell.execute_reply":"2024-08-05T12:27:15.964962Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"datasets.arrow_dataset.Dataset"},"metadata":{}}]},{"cell_type":"code","source":"# Load dataset\ntrain_dataset = TextDataset(Args.trainDataset, tokenizer, context_len=Args.context_len)\neval_dataset = TextDataset(Args.testDataset, tokenizer, context_len=Args.context_len)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=Args.train_batch_size, shuffle=False)\neval_dataloader = DataLoader(eval_dataset, batch_size=Args.valid_batch_size, shuffle=False)\n\n# Optimizer and Scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=Args.lr)\nscheduler = get_scheduler(\n    \"cosine\", optimizer=optimizer, num_warmup_steps=1, num_training_steps=len(train_dataloader) * Args.epochs\n)","metadata":{"id":"GlK6PJ-N6u6c","execution":{"iopub.status.busy":"2024-08-05T12:27:16.720776Z","iopub.execute_input":"2024-08-05T12:27:16.721817Z","iopub.status.idle":"2024-08-05T12:27:20.585157Z","shell.execute_reply.started":"2024-08-05T12:27:16.721778Z","shell.execute_reply":"2024-08-05T12:27:20.583986Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"scheduler ","metadata":{"execution":{"iopub.status.busy":"2024-08-05T12:27:20.587546Z","iopub.execute_input":"2024-08-05T12:27:20.588049Z","iopub.status.idle":"2024-08-05T12:27:20.595349Z","shell.execute_reply.started":"2024-08-05T12:27:20.588007Z","shell.execute_reply":"2024-08-05T12:27:20.594215Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<torch.optim.lr_scheduler.LambdaLR at 0x7fd25b2dd3f0>"},"metadata":{}}]},{"cell_type":"code","source":"model.to(Args.device)\nfor epoch in range(Args.epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in train_dataloader:\n        batch = {k: v.to(Args.device) for k, v in batch.items()}\n\n        outputs = model(batch['input_ids'])\n\n        # Compute the loss manually\n        shift_logits = outputs[..., :-1, :].contiguous()\n        shift_labels = batch['labels'][..., 1:].contiguous()\n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{Args.epochs}, Loss: {total_loss / len(train_dataloader)}\")\n    # Evaluation\n    model.eval()\n    total_eval_loss = 0\n\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            batch = {k: v.to(Args.device) for k, v in batch.items()}\n\n            outputs = model(batch['input_ids'])\n\n            # Compute the loss manually\n            shift_logits = outputs[..., :-1, :].contiguous()\n            shift_labels = batch['labels'][..., 1:].contiguous()\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n            total_eval_loss += loss.item()\n\n    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n    print(f\"Epoch {epoch+1}/{Args.epochs}, Evaluation Loss: {avg_eval_loss}\")\n    model_save_path = \"mamba_darija.pt\"\n    torch.save(model.state_dict(), model_save_path)\n    print(\"Training complete!\")","metadata":{"id":"yZDqNqCtwyJe","execution":{"iopub.status.busy":"2024-08-05T12:27:20.596939Z","iopub.execute_input":"2024-08-05T12:27:20.597380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"wQfK9nsp-56D"},"execution_count":null,"outputs":[]}]}